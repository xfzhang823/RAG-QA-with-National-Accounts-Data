{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6> About this File:<br>\n",
    "<font size = 3.5> A simple RAG (Retrieval-Augmented Generation). <br>\n",
    "Documents and tools:\n",
    "* retrieval knowledge base: the SNA 2008 manual (System of National Accounts)\n",
    "* LLM: ChatGPT 3.5 Instruct\n",
    "* Embedding model: \"text-embedding-ada-002\" (ChatGPT)\n",
    "* Vector database: pinecone (serverless)\n",
    "\n",
    "<br><br>Example query: \"_In what situations do national accounts consolidate accounts?_\", <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github version\n",
    "\n",
    "import os\n",
    "\n",
    "PINECONE_API_KEY = os.environ[\"api_key_openai\"]\n",
    "OPENAI_API_KEY = os.environ[\"api_key_pinecone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pdfplumber\n",
    "import pypdf\n",
    "import PyPDF2\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse SNA Manual - PDF Document - with PDF reader libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try few different parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = r\"C:\\Users\\xzhan\\Documents\\Vertical Definition and Views\\SNA 2008 Manual.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pdfplumber<br>\n",
    "This one has problem with dual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By page; this one does not handle dual column; it's a no go\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "pdf_path = r\"C:\\Users\\xzhan\\Documents\\Vertical Definition and Views\\SNA 2008 Manual.pdf\"\n",
    "text_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_text.txt\"\n",
    "\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in pdf.pages:\n",
    "\n",
    "        # Extract text from each page and append it to the full_text string\n",
    "\n",
    "        text = page.extract_text()\n",
    "\n",
    "        if text:  # Checking if text extraction returned any content\n",
    "\n",
    "            full_text += text + \" \"\n",
    "\n",
    "\n",
    "with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pages: 100%|██████████| 722/722 [01:36<00:00,  7.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# By line and removing \\n; this one does not handle dual column; it's a no go\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in tqdm(pdf.pages, desc=\"Processing Pages\"):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            # Replace singular newline characters with a space, assuming double newlines are paragraph breaks\n",
    "            text = text.replace(\n",
    "                \"\\n\", \" \"\n",
    "            )  # This replaces all newlines; might need more logic for double newlines\n",
    "            full_text += text + \" \"\n",
    "\n",
    "with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnstructuredPDFLoader\n",
    "* Too many dependencies; not worth the effort given the current goals (more just texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many dependencies, not worth doing it now.\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "pdf_path = PDF_PATH\n",
    "loader = UnstructuredPDFLoader(pdf_path)\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pypdf (PyPFD 1 version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "text_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_text.txt\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pypdf2\n",
    "* Ended up using this one b/c it can handle dual column layout in the pdf file\n",
    "* Relatively easy to use and stable\n",
    "* Cons: some words were separated by \" \" - pdf layout caused issues; could not extract table data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_path = PDF_PATH  # Update this to the path of your PDF file\n",
    "text_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_text_pypdf2.txt\"\n",
    "\n",
    "# Open the PDF file\n",
    "with open(pdf_path, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = []\n",
    "\n",
    "    # Iterate over each page and extract text\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text.append(page.extract_text())  # extract_text() is the updated method name\n",
    "\n",
    "    # Combine the text of all pages into a single string\n",
    "    full_text = \"\\n\".join(text)\n",
    "\n",
    "# Now you can print the text or save it to a file\n",
    "with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning \"......\"\n",
    "\n",
    "cleaned_lines = []\n",
    "with open(text_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        # Your line processing logic\n",
    "        if not any(char.isalnum() for char in line) or line.count(\".\") > 5:\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "with open(text_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the line breaks to form a single blob of texts\n",
    "cleaned_text = \"\"\n",
    "with open(text_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        # Remove unwanted lines based on your previous criteria\n",
    "        if not any(char.isalnum() for char in line) or line.count(\".\") > 5:\n",
    "            continue\n",
    "        # Strip trailing newline characters and whitespace, then concatenate\n",
    "        cleaned_text += line.strip() + \" \"  # Add a space for word separation\n",
    "\n",
    "# Now cleaned_text contains all the cleaned lines stitched together\n",
    "# Optionally, you can save this back to a file\n",
    "with open(text_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse text into sentences via Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy split into sententences & save to a file - Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional, if you want to do this more in multiple discrete steps\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the path for your input and output files\n",
    "input_file_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_text_pypdf2.txt\"\n",
    "output_file_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_sents.txt\"\n",
    "\n",
    "\n",
    "# Read the large text file in chunks (you can adjust the chunk size)\n",
    "def read_in_chunks(file_path, chunk_size=1000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "        # the while loop creates an infinite loop.\n",
    "        # This loop will continuously execute its block of code until it encounters a break statement or another form of interruption.\n",
    "        # This condition becomes true when file.read(chunk_size) reaches the end of the file and returns an empty string.\n",
    "\n",
    "\n",
    "# Initialize a progress bar\n",
    "file_size = sum(1 for _ in open(input_file_path, \"r\", encoding=\"utf-8\"))\n",
    "# The sum(1 for ...) iterates over each line in the input file and sums up a count of 1 for each line,\n",
    "# counting the total number of lines in the file.\n",
    "# \"each line\" refers to a single line of text in the input file, as separated by newline characters (\\n).\n",
    "\n",
    "pbar = tqdm(total=file_size, desc=\"Processing\")\n",
    "# The value is used to set the maximum value (total) for the progress bar.\n",
    "\n",
    "\n",
    "# Process the text in batches and write sentences to the output file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for text_chunk in read_in_chunks(input_file_path):\n",
    "        # Processing the chunk with spaCy\n",
    "        for doc in nlp.pipe(\n",
    "            [text_chunk], batch_size=100\n",
    "        ):  # Adjust batch_size as needed\n",
    "            for sent in doc.sents:\n",
    "\n",
    "                output_file.write(sent.text + \"\\n\")\n",
    "\n",
    "        pbar.update(len(text_chunk))  # Update progress after each chunk is processed\n",
    "\n",
    "\n",
    "pbar.close()  # Close the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional; quick count of how many lines of sentences\n",
    "\n",
    "\n",
    "def count_lines(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return len(lines)\n",
    "\n",
    "\n",
    "print(count_lines(output_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy split into sents, add IDs, & save to a json file format (pinecone ready format)<br>\n",
    "* About spaCy: developed by MIT, spaCy is a library for Natural Language Processing in Python. I am using its quick text processing feature to parse the text into sentences. This is more of a personal choice. spaCy can condenses few lines of codes into one or two lines of code with multiple generators. However, it is more \"high maintenance\" when it comes to installation and maintenance... a lot pip re-installs over the years. Therefore, the NLTK library, by Columbia University, is probably safer to use, as it is more popular and \"stable\". \n",
    "*  I included an extra step to add an \"sent-id-xxx\" field. Although adding ids is not required, but it's a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 22982323it [01:01, 375110.93it/s]    \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the path for the input and output files\n",
    "input_file_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_text_pypdf2.txt\"\n",
    "output_json_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_pinecone_data.json\"\n",
    "\n",
    "\n",
    "# Read the large text file in chunks (you can adjust the chunk size in the function)\n",
    "def read_in_chunks(file_path, chunk_size=1000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "        # the while loop creates an infinite loop.\n",
    "        # This loop will continuously execute its block of code until it encounters a break statement or another form of interruption.\n",
    "        # This condition becomes true when file.read(chunk_size) reaches the end of the file and returns an empty string.\n",
    "\n",
    "\n",
    "# Initialize a progress bar\n",
    "total_progress = sum(1 for _ in open(input_file_path, \"r\", encoding=\"utf-8\"))\n",
    "# The sum(1 for ...) iterates over each line in the input file and sums up a count of 1 for each line,\n",
    "# counting the total number of lines in the file.\n",
    "# \"each line\" refers to a single line of text in the input file, as separated by newline characters (\\n).\n",
    "pbar = tqdm(total=total_progress, desc=\"Processing\")\n",
    "# The value is used to set the maximum value (total) for the progress bar.\n",
    "\n",
    "# Process the text in batches load into a variable\n",
    "all_sents = []\n",
    "for text_chunk in read_in_chunks(input_file_path):\n",
    "    # Processing the chunk with spaCy\n",
    "    for doc in nlp.pipe([text_chunk], batch_size=100):  # Adjust batch_size as needed\n",
    "        for sent in doc.sents:\n",
    "            all_sents.append(sent.text.strip())\n",
    "            pbar.update(\n",
    "                len(text_chunk)\n",
    "            )  # Update progress after each chunk is processed\n",
    "\n",
    "# Adjust the progress bar's total to account for the remaining steps (saving to JSON)\n",
    "additional_steps = len(\n",
    "    all_sents\n",
    ")  # Assuming each sentence will count as a step in the progress\n",
    "pbar.total += additional_steps\n",
    "\n",
    "# Create a list of dictionaries for Pinecone upsert\n",
    "pinecone_data = [\n",
    "    {\"id\": f\"sent-id-{i + 1}\", \"text\": sentence} for i, sentence in enumerate(all_sents)\n",
    "]\n",
    "pbar.update(additional_steps)  # Update progress for appending sentences\n",
    "\n",
    "# save data to a JSON file\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(pinecone_data, json_file)\n",
    "    pbar.update(len(all_sents))\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data and Chunk<br>\n",
    "* Pinecone vector database requires at least id (unique identifier) and value (vector values, i.e. [0.5, 1, 1.2...]) to upload. Therefore, we must generate unique ids for the \"chunked\" texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5568c830f54f22876715acb3b4f115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "json_path = r\"C:\\github\\chatgpt\\sna_manual_parsed_pinecone_data.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "window = 10  # No. of sentences to combine\n",
    "stride = 2  # No. of sentences to 'stride' over\n",
    "# (you want some \"buffer\" before and after each \"text snippet.\"\n",
    "# Snippet 2 overlaps with snippet 1, and 3 some overlap with 2, so on...\n",
    "# The redundancy makes earch snippet a bit richer.)\n",
    "\n",
    "chunked_data = []  # Store the resulting chunks here\n",
    "\n",
    "for i in tqdm(range(0, len(data), stride)):\n",
    "    i_end = min(len(data), i + window)\n",
    "    chunk_text = \" \".join(\n",
    "        item[\"text\"] for item in data[i:i_end]\n",
    "    )  # Aggregate text within the window\n",
    "\n",
    "    # Create and append the chunked data\n",
    "    # Assign a new ID or use the ID of the first sentence in the chunk\n",
    "    chunk_id = f\"chunk-id-{i // stride}\"\n",
    "    chunked_data.append({\"id\": chunk_id, \"text\": chunk_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11469"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and upsert (load) into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pinecone (vector dbs/vecotor store)\n",
    "\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "# pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag-qa-sna-manual\"  # name of the vector dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate pinecone index (vector dbs) (create a new one if we have not yet built one)\n",
    "\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\"),\n",
    "    )\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert data into pinecone\n",
    "* For our purpose, we also need text associated with the values in pinecone's metadata (we use the vector to search, but it's the snippets of document texts that we need.) Therefore, we need to include text on top of ids and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57923d502e04c87b8f354e80227588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the version modified by chatgpt\n",
    "# This part takes few minutes, depending on your hardware\n",
    "\n",
    "import openai\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "data = chunked_data\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)\n",
    "    meta_batch = data[i:i_end]\n",
    "\n",
    "    ids_batch = [x[\"id\"] for x in meta_batch]\n",
    "    texts = [x[\"text\"] for x in meta_batch]\n",
    "\n",
    "    embeds = []\n",
    "    attempt = 0\n",
    "    max_attempts = 5\n",
    "    while not embeds and attempt < max_attempts:\n",
    "        try:\n",
    "            res = openai.embeddings.create(input=texts, model=embed_model)\n",
    "            embeds = [embedding.embedding for embedding in res.data]\n",
    "            # [embedding[\"embedding\"] for embedding in res.data]\n",
    "        except Exception as e:  # Catching any exception\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            # Implementing a simple backoff strategy, assuming error might be rate-limiting or similar\n",
    "            sleep_time = 60 * (2**attempt)\n",
    "            time.sleep(sleep_time)\n",
    "            attempt += 1\n",
    "\n",
    "    if not embeds:\n",
    "        print(f\"Failed to create embeddings after {max_attempts} attempts.\")\n",
    "        continue\n",
    "\n",
    "    to_upsert = [\n",
    "        (id, embed, {\"text\": text}) for id, embed, text in zip(ids_batch, embeds, texts)\n",
    "    ]  # this upserts id, values (vectors), and text\n",
    "\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query augmented by retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In what situations do national accounts consolidate accounts?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query without retrieval augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "National accounts typically consolidate accounts in the following situations:\n",
      "\n",
      "1. Government accounts: National accounts consolidate the accounts of different levels of government (federal, state, and local) to provide a comprehensive view of the country's public finances.\n",
      "\n",
      "2. International trade: National accounts consolidate the accounts of imports and exports to measure a country's balance of trade and current account balance.\n",
      "\n",
      "3. Financial sector: National accounts consolidate the accounts of financial institutions, such as banks and insurance companies, to measure the country's financial sector's contribution to the economy.\n",
      "\n",
      "4. Public and private sectors: National accounts consolidate the accounts of both the public and private sectors to provide a complete picture of the country's economic activity.\n",
      "\n",
      "5. National income: National accounts consolidate the accounts of different sectors of the economy (households, businesses, government) to measure the country's national income and gross domestic product (GDP).\n",
      "\n",
      "6. International comparisons: National accounts consolidate accounts to facilitate international comparisons of economic performance and to assess a country's position in the global economy.\n",
      "\n",
      "7. Time series analysis: National accounts consolidate accounts over time to track changes in the economy and identify trends and patterns.\n",
      "\n",
      "8. Policy analysis: National accounts consolidate accounts to inform policy decisions and evaluate the effectiveness of economic policies.\n",
      "\n",
      "9. Macroeconomic analysis: National accounts consolidate accounts to analyze the overall performance of the economy, including factors such as inflation, employment, and economic growth.\n",
      "\n",
      "10. Reporting and transparency: National accounts consolidate accounts to provide a transparent and standardized view of the country's economic activity, which is essential for investors, policymakers, and other stakeholders.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "openai.models.list()\n",
    "\n",
    "\n",
    "def complete(prompt):\n",
    "    res = openai.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        stop=None,\n",
    "    )\n",
    "    return res.choices[0].text.strip()\n",
    "\n",
    "\n",
    "answer_no_rag = complete(query)\n",
    "print(answer_no_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a query vector, search the retrieval vector dbs (pinecone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to search the retrieval vector dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = openai.embeddings.create(input=[query], model=embed_model)\n",
    "\n",
    "# retrieve the embedding\n",
    "xq = res.data[0].embedding\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "index = pc.Index(index_name)\n",
    "res = index.query(vector=xq, top_k=2, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'chunk-id-590',\n",
       "              'metadata': {'text': 'As a matter of principle, flows between '\n",
       "                                   'constituent units within subsectors or  '\n",
       "                                   'sectors are not consolidated. However, '\n",
       "                                   'consolidated account s may be compiled for '\n",
       "                                   'complementary presentations and analyses. '\n",
       "                                   'Even then, transactionsappearing in '\n",
       "                                   'different accounts are never consolidated '\n",
       "                                   'so that the balancing items are not '\n",
       "                                   'affected by consolidation. Consolidation '\n",
       "                                   'may be useful, for example, for the '\n",
       "                                   'government sector as a whole, thus showing '\n",
       "                                   'the net relations between g overnment and '\n",
       "                                   'the rest of the economy . This possibility '\n",
       "                                   'is elaborated in chapter 22. 2.70 Accounts '\n",
       "                                   'for the total econom y, when fully '\n",
       "                                   'consolidated, give rise to the rest of the '\n",
       "                                   'world account (external transactions '\n",
       "                                   'account). Overview 23Netting 2.71 '\n",
       "                                   'Consolidation must be di stinguished from '\n",
       "                                   'netting. For current transactions, netting '\n",
       "                                   'refers to offsetting uses against '\n",
       "                                   'resources. The SNA does th is only in a '\n",
       "                                   'few specific instances; for example, taxes '\n",
       "                                   'on products may be shown net of subsidies '\n",
       "                                   'on products.'},\n",
       "              'score': 0.874295413,\n",
       "              'values': []},\n",
       "             {'id': 'chunk-id-591',\n",
       "              'metadata': {'text': 'Even then, transactionsappearing in '\n",
       "                                   'different accounts are never consolidated '\n",
       "                                   'so that the balancing items are not '\n",
       "                                   'affected by consolidation. Consolidation '\n",
       "                                   'may be useful, for example, for the '\n",
       "                                   'government sector as a whole, thus showing '\n",
       "                                   'the net relations between g overnment and '\n",
       "                                   'the rest of the economy . This possibility '\n",
       "                                   'is elaborated in chapter 22. 2.70 Accounts '\n",
       "                                   'for the total econom y, when fully '\n",
       "                                   'consolidated, give rise to the rest of the '\n",
       "                                   'world account (external transactions '\n",
       "                                   'account). Overview 23Netting 2.71 '\n",
       "                                   'Consolidation must be di stinguished from '\n",
       "                                   'netting. For current transactions, netting '\n",
       "                                   'refers to offsetting uses against '\n",
       "                                   'resources. The SNA does th is only in a '\n",
       "                                   'few specific instances; for example, taxes '\n",
       "                                   'on products may be shown net of subsidies '\n",
       "                                   'on products. For changes in assets or '\n",
       "                                   'changes in liabilities, netting may be  '\n",
       "                                   'envisaged in two ways. The first case is '\n",
       "                                   'where various t ypes of changes in assets '\n",
       "                                   '(for example, entries in invent ories and '\n",
       "                                   'withdrawals from inventories) or various '\n",
       "                                   'types of liabilities (for example, '\n",
       "                                   'incurrence of a new debt a nd redemption '\n",
       "                                   'of an existing debt) are netted.'},\n",
       "              'score': 0.868941367,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search retrieval, augment, and query again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited by ChatGPT\n",
    "limit = 5000\n",
    "\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.embeddings.create(input=[query], model=embed_model)\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res.data[0].embedding\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "    contexts = [x[\"metadata\"][\"text\"] for x in res[\"matches\"]]\n",
    "\n",
    "    # build your pompt w/t the retreived context included\n",
    "    prompt_start = \"Answer the question based on the context below.\\n\\n\" + \"Context:\\n\"\n",
    "    prompt_end = f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # # Initialize prompt to avoid UnboundLocalError\n",
    "    # prompt = prompt_start + prompt_end\n",
    "\n",
    "    # append contexts until hitting limit\n",
    "    prompt = prompt_start\n",
    "    for context in contexts:\n",
    "        if len(prompt + context + \"\\n\\n---\\n\\n\" + prompt_end) > limit:\n",
    "            break\n",
    "        prompt += context + \"\\n\\n---\\n\\n\"\n",
    "    prompt += prompt_end\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on teh context below.\n",
      "\n",
      "Context:\n",
      "As a matter of principle, flows between constituent units within subsectors or  sectors are not consolidated. However, consolidated account s may be compiled for complementary presentations and analyses. Even then, transactionsappearing in different accounts are never consolidated so that the balancing items are not affected by consolidation. Consolidation may be useful, for example, for the government sector as a whole, thus showing the net relations between g overnment and the rest of the economy . This possibility is elaborated in chapter 22. 2.70 Accounts for the total econom y, when fully consolidated, give rise to the rest of the world account (external transactions account). Overview 23Netting 2.71 Consolidation must be di stinguished from netting. For current transactions, netting refers to offsetting uses against resources. The SNA does th is only in a few specific instances; for example, taxes on products may be shown net of subsidies on products.\n",
      "\n",
      "---\n",
      "\n",
      "Even then, transactionsappearing in different accounts are never consolidated so that the balancing items are not affected by consolidation. Consolidation may be useful, for example, for the government sector as a whole, thus showing the net relations between g overnment and the rest of the economy . This possibility is elaborated in chapter 22. 2.70 Accounts for the total econom y, when fully consolidated, give rise to the rest of the world account (external transactions account). Overview 23Netting 2.71 Consolidation must be di stinguished from netting. For current transactions, netting refers to offsetting uses against resources. The SNA does th is only in a few specific instances; for example, taxes on products may be shown net of subsidies on products. For changes in assets or changes in liabilities, netting may be  envisaged in two ways. The first case is where various t ypes of changes in assets (for example, entries in invent ories and withdrawals from inventories) or various types of liabilities (for example, incurrence of a new debt a nd redemption of an existing debt) are netted.\n",
      "\n",
      "---\n",
      "\n",
      "- temporal comparisons because countries at different stages of development are involved. 2.67 Both inter-temporal and inte rspatial measures are discussed in chapter 15. 4. Consolidation and netting Consolidation 2.68 Consolidation may cover va rious accounting procedures. In general, it refers to the elimination from both uses and resources of transactions wh ich occur between units that are grouped together and to th e elimination of financial assets and the count erpart liabilities. 2.69 As a matter of principle, flows between constituent units within subsectors or  sectors are not consolidated. However, consolidated account s may be compiled for complementary presentations and analyses. Even then, transactionsappearing in different accounts are never consolidated so that the balancing items are not affected by consolidation. Consolidation may be useful, for example, for the government sector as a whole, thus showing the net relations between g overnment and the rest of the economy\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "Question: In what situations do national accounts consolidate accounts?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "query_with_contexts = retrieve(query)\n",
    "print(query_with_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "National accounts consolidate accounts in situations where it is useful for complementary presentations and analyses, such as for the government sector as a whole, to show the net relations between government and the rest of the economy.\n"
     ]
    }
   ],
   "source": [
    "# complete the context-infused query\n",
    "answer_rag = complete(query_with_contexts)\n",
    "print(answer_rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
